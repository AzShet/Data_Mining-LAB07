{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe74aee5",
   "metadata": {},
   "source": [
    "# LABORATORIO SEMANA 7\n",
    "Realizado por:\n",
    "\n",
    "**- Ruelas Flores, César Diego**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26ebc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa009860",
   "metadata": {},
   "source": [
    "**Mi metodo tradicional de cargar datos de ucilmrepo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d943a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "breast_cancer_wisconsin_original = fetch_ucirepo(id=15) \n",
    "X = breast_cancer_wisconsin_original.data.features \n",
    "y = breast_cancer_wisconsin_original.data.targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d32601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar en un DataFrame\n",
    "data_directa_ucimlrepo = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa52fe",
   "metadata": {},
   "source": [
    "#### **Variables Globales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec97e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\"\n",
    "COLUMN_NAMES = ['id', 'clump_thickness', 'uniformity_cell_size', 'uniformity_cell_shape',\n",
    "                'marginal_adhesion', 'single_epithelial_cell_size', 'bare_nuclei',\n",
    "                'bland_chromatin', 'normal_nucleoli', 'mitoses', 'class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3741fd96",
   "metadata": {},
   "source": [
    "## FUNCIONES DE UTILIDAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce6596",
   "metadata": {},
   "source": [
    "Función para cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e05b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_url(url):\n",
    "    \"\"\"\n",
    "    Retrieve data from a CSV into a dataframe.\n",
    "\n",
    "    input:\n",
    "    - url: URL of the csv file\n",
    "\n",
    "    output:\n",
    "    - dataframe: pd.DataFrame\n",
    "    \"\"\"\n",
    "    return pd.read_csv(url, names=COLUMN_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c8f0e",
   "metadata": {},
   "source": [
    "Limpieza del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5da96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean the dataset by handling missing values, converting types, \n",
    "    and adjusting the target column.\n",
    "\n",
    "    input:\n",
    "    - df: Raw dataframe\n",
    "\n",
    "    output:\n",
    "    - df: Cleaned dataframe\n",
    "    \"\"\"\n",
    "    df = df.replace('?', np.nan)\n",
    "    df = df.dropna()\n",
    "    df['bare_nuclei'] = df['bare_nuclei'].astype(int)\n",
    "    df['class'] = df['class'].apply(lambda x: 1 if x == 4 else 0)\n",
    "    df = df.drop(columns=['id'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec589e33",
   "metadata": {},
   "source": [
    "Cálculo de IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ebed750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iv(df, target):\n",
    "    \"\"\"\n",
    "    Calculate Information Value (IV) for all features in the dataset.\n",
    "\n",
    "    input:\n",
    "    - df: Cleaned dataframe\n",
    "    - target: Target variable name (str)\n",
    "\n",
    "    output:\n",
    "    - pd.Series: IV values per feature\n",
    "    \"\"\"\n",
    "    def woe_iv(df, feature, target):\n",
    "        lst = []\n",
    "        for val in np.sort(df[feature].unique()):\n",
    "            count_event = ((df[feature] == val) & (df[target] == 1)).sum()\n",
    "            count_non_event = ((df[feature] == val) & (df[target] == 0)).sum()\n",
    "            lst.append([val, count_event, count_non_event])\n",
    "        data = pd.DataFrame(lst, columns=['Value', 'Event', 'NonEvent'])\n",
    "        data['Dist_Event'] = data['Event'] / data['Event'].sum()\n",
    "        data['Dist_NonEvent'] = data['NonEvent'] / data['NonEvent'].sum()\n",
    "        data['WOE'] = np.log(data['Dist_Event'] / data['Dist_NonEvent'])\n",
    "        data['IV'] = (data['Dist_Event'] - data['Dist_NonEvent']) * data['WOE']\n",
    "        return data['IV'].sum()\n",
    "\n",
    "    iv_dict = {col: woe_iv(df, col, target) for col in df.columns if col != target}\n",
    "    return pd.Series(iv_dict).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06576616",
   "metadata": {},
   "source": [
    "El mismo codigo de arriba pero optimizado (con IA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e02281a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_iv_optimized(df: pd.DataFrame, target: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calcula el Information Value (IV) para todas las columnas/características en el dataset.\n",
    "\n",
    "    Entrada:\n",
    "    - df: DataFrame (pd.DataFrame) limpio con columnas y variable objetivo binaria (0/1).\n",
    "    - target: Nombre de la columna objetivo (str).\n",
    "\n",
    "    Salida:\n",
    "    - pd.Series: Valores de IV por columna/característica, ordenados descendentemente.\n",
    "    \"\"\"\n",
    "    def woe_iv_optimized_single_feature(df: pd.DataFrame, feature: str, target: str) -> float:\n",
    "        counts = df.groupby([feature, target]).size()\n",
    "        count_table = counts.unstack(fill_value=0)\n",
    "\n",
    "        if 0 not in count_table.columns:\n",
    "             count_table[0] = 0\n",
    "        if 1 not in count_table.columns:\n",
    "             count_table[1] = 1\n",
    "\n",
    "        count_table = count_table[[0, 1]]\n",
    "        count_table.columns = ['NonEvent', 'Event']\n",
    "\n",
    "        total_event = count_table['Event'].sum()\n",
    "        total_non_event = count_table['NonEvent'].sum()\n",
    "\n",
    "        if total_event == 0 or total_non_event == 0:\n",
    "            return 0.0\n",
    "\n",
    "        epsilon = 1e-6\n",
    "        count_table['Dist_Event'] = count_table['Event'] / total_event\n",
    "        count_table['Dist_NonEvent'] = count_table['NonEvent'] / total_non_event\n",
    "\n",
    "        count_table['Dist_Event'] = count_table['Dist_Event'].replace(0, epsilon)\n",
    "        count_table['Dist_NonEvent'] = count_table['Dist_NonEvent'].replace(0, epsilon)\n",
    "\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "             count_table['WOE'] = np.log(count_table['Dist_Event'] / count_table['Dist_NonEvent'])\n",
    "\n",
    "        count_table.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "        count_table['IV'] = (count_table['Dist_Event'] - count_table['Dist_NonEvent']) * count_table['WOE']\n",
    "\n",
    "        feature_iv = count_table['IV'].sum()\n",
    "\n",
    "        return feature_iv\n",
    "\n",
    "    iv_dict = {col: woe_iv_optimized_single_feature(df, col, target)\n",
    "               for col in df.columns if col != target}\n",
    "\n",
    "    return pd.Series(iv_dict).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c76f4",
   "metadata": {},
   "source": [
    "Selección de variables fuertes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ae2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_strong_predictors(iv_series, threshold=0.02):\n",
    "    \"\"\"\n",
    "    Select predictors with strong IV above the threshold.\n",
    "\n",
    "    input:\n",
    "    - iv_series: Series with IV per variable\n",
    "    - threshold: Minimum IV to consider strong\n",
    "\n",
    "    output:\n",
    "    - list: Selected features\n",
    "    \"\"\"\n",
    "    return iv_series[iv_series >= threshold].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc78c56",
   "metadata": {},
   "source": [
    "## A. Calcular IV, excluir variables débiles, separar target y dividir en entrenamiento/prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50482168",
   "metadata": {},
   "source": [
    "**Cargar, limpiar y preparar datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1af218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data_from_url(DATA_URL)\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa3b12a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Value por variable:\n",
      " clump_thickness                inf\n",
      "uniformity_cell_size           inf\n",
      "uniformity_cell_shape          inf\n",
      "marginal_adhesion              inf\n",
      "single_epithelial_cell_size    inf\n",
      "bare_nuclei                    inf\n",
      "bland_chromatin                inf\n",
      "normal_nucleoli                inf\n",
      "mitoses                        inf\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "iv_series = calculate_iv(df, 'class')\n",
    "print(\"Information Value por variable:\\n\", iv_series)\n",
    "SELECTED_FEATURES = select_strong_predictors(iv_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b18fae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Value por variable:\n",
      " uniformity_cell_size           10.388630\n",
      "uniformity_cell_shape           8.601120\n",
      "clump_thickness                 6.532306\n",
      "bland_chromatin                 6.286301\n",
      "normal_nucleoli                 6.085052\n",
      "bare_nuclei                     5.463221\n",
      "marginal_adhesion               4.880258\n",
      "single_epithelial_cell_size     4.235413\n",
      "mitoses                         2.331167\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "iv_series = calculate_iv_optimized(df, 'class')\n",
    "print(\"Information Value por variable:\\n\", iv_series)\n",
    "SELECTED_FEATURES = select_strong_predictors(iv_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d3551",
   "metadata": {},
   "source": [
    "**DIFERENCIAS????**\n",
    "\n",
    "Esto ocurre por lo siguiente:\n",
    "\n",
    "Mi código original produce inf porque no maneja explícitamente el caso donde una categoría de una columna tiene **cero observaciones para una clase objetivo**, lo que lleva a un **log(0) o una división por cero en la fórmula del WOE**. Sin embargo **el código optimizado añade un pequeño valor (epsilon) a las distribuciones que son cero antes de tomar el logaritmo, evitando el resultado infinito y proporcionando un valor numérico finito para el IV.** Este manejo de ceros es una **práctica estándar para hacer el cálculo de IV más robusto**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70400dc2",
   "metadata": {},
   "source": [
    "Recordemos que para saber qué tan buena es una columna para predecir nuestro objetivo objetivo (si el cáncer es benigno o maligno). Un **IV alto significa que la columna es muy útil para predecir** y si tenemos el resultado infinito, como que al final no nos es tan util a primera vista, con esto concluyo en que el calculo del IV de manera optimizada es la mejor para esta situacion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf63b83",
   "metadata": {},
   "source": [
    "**Separar entrenamiento y prueba**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcc39706",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[SELECTED_FEATURES]\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29019d3",
   "metadata": {},
   "source": [
    "## B. Modelo de regresión logística + significancia + métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf8c71",
   "metadata": {},
   "source": [
    "### Significancia estadística con statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "646f3d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.073760\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  class   No. Observations:                  512\n",
      "Model:                          Logit   Df Residuals:                      502\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Sat, 03 May 2025   Pseudo R-squ.:                  0.8842\n",
      "Time:                        20:36:48   Log-Likelihood:                -37.765\n",
      "converged:                       True   LL-Null:                       -326.13\n",
      "Covariance Type:            nonrobust   LLR p-value:                2.070e-118\n",
      "===============================================================================================\n",
      "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "const                         -10.0640      1.408     -7.150      0.000     -12.823      -7.305\n",
      "uniformity_cell_size           -0.0695      0.245     -0.283      0.777      -0.550       0.411\n",
      "uniformity_cell_shape           0.3711      0.249      1.492      0.136      -0.117       0.859\n",
      "clump_thickness                 0.4874      0.157      3.098      0.002       0.179       0.796\n",
      "bland_chromatin                 0.5931      0.217      2.738      0.006       0.169       1.018\n",
      "normal_nucleoli                 0.1494      0.127      1.173      0.241      -0.100       0.399\n",
      "bare_nuclei                     0.4098      0.103      3.963      0.000       0.207       0.613\n",
      "marginal_adhesion               0.2079      0.141      1.478      0.140      -0.068       0.484\n",
      "single_epithelial_cell_size     0.0438      0.189      0.232      0.817      -0.326       0.414\n",
      "mitoses                         0.4549      0.341      1.334      0.182      -0.213       1.123\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "X_train_sm = sm.add_constant(X_train.astype(float))\n",
    "logit_model = sm.Logit(y_train, X_train_sm).fit()\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f665e37",
   "metadata": {},
   "source": [
    "### Modelo con sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0365beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=200)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ae96a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Métricas Regresión Logística ---\n",
      "Precisión: 0.9532163742690059\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       103\n",
      "           1       0.98      0.90      0.94        68\n",
      "\n",
      "    accuracy                           0.95       171\n",
      "   macro avg       0.96      0.94      0.95       171\n",
      "weighted avg       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Métricas Regresión Logística ---\")\n",
    "print(\"Precisión:\", metrics.accuracy_score(y_test, y_pred_lr))\n",
    "print(metrics.classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b6018",
   "metadata": {},
   "source": [
    "## C. Modelo SVM y comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d6dd4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2228bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b21a0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e0a02305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Métricas SVM ---\n",
      "Precisión SVM: 0.9590643274853801\n",
      "Reporte de clasificación SVM:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97       103\n",
      "           1       0.97      0.93      0.95        68\n",
      "\n",
      "    accuracy                           0.96       171\n",
      "   macro avg       0.96      0.95      0.96       171\n",
      "weighted avg       0.96      0.96      0.96       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Métricas SVM ---\")\n",
    "print(\"Precisión SVM:\", metrics.accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Reporte de clasificación SVM:\\n\", metrics.classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5906766d",
   "metadata": {},
   "source": [
    "## **Comparación**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fb5b75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparación de modelos:\n",
      "Logistic Regression Accuracy: 0.9532\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparación de modelos:\")\n",
    "print(f\"Logistic Regression Accuracy: {metrics.accuracy_score(y_test, y_pred_lr):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0962b3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9474\n"
     ]
    }
   ],
   "source": [
    "print(f\"SVM Accuracy: {metrics.accuracy_score(y_test, y_pred_svm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af679ee",
   "metadata": {},
   "source": [
    "## Pytest ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cb282592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main_module.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main_module.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean the dataset by handling missing values, converting types, \n",
    "    and adjusting the target column.\n",
    "\n",
    "    input:\n",
    "    - df: Raw dataframe\n",
    "\n",
    "    output:\n",
    "    - df: Cleaned dataframe\n",
    "    \"\"\"\n",
    "    df = df.replace('?', np.nan)\n",
    "    df = df.dropna()\n",
    "    df['bare_nuclei'] = df['bare_nuclei'].astype(int)\n",
    "    df['class'] = df['class'].apply(lambda x: 1 if x == 4 else 0)\n",
    "    df = df.drop(columns=['id'])\n",
    "    return df\n",
    "\n",
    "def calculate_iv_optimized(df: pd.DataFrame, target: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calcula el Information Value (IV) para todas las columnas/características en el dataset.\n",
    "\n",
    "    Entrada:\n",
    "    - df: DataFrame (pd.DataFrame) limpio con columnas y variable objetivo binaria (0/1).\n",
    "    - target: Nombre de la columna objetivo (str).\n",
    "\n",
    "    Salida:\n",
    "    - pd.Series: Valores de IV por columna/característica, ordenados descendentemente.\n",
    "    \"\"\"\n",
    "    def woe_iv_optimized_single_feature(df: pd.DataFrame, feature: str, target: str) -> float:\n",
    "        counts = df.groupby([feature, target]).size()\n",
    "        count_table = counts.unstack(fill_value=0)\n",
    "\n",
    "        if 0 not in count_table.columns:\n",
    "             count_table[0] = 0\n",
    "        if 1 not in count_table.columns:\n",
    "             count_table[1] = 1\n",
    "\n",
    "        count_table = count_table[[0, 1]]\n",
    "        count_table.columns = ['NonEvent', 'Event']\n",
    "\n",
    "        total_event = count_table['Event'].sum()\n",
    "        total_non_event = count_table['NonEvent'].sum()\n",
    "\n",
    "        if total_event == 0 or total_non_event == 0:\n",
    "            return 0.0\n",
    "\n",
    "        epsilon = 1e-6\n",
    "        count_table['Dist_Event'] = count_table['Event'] / total_event\n",
    "        count_table['Dist_NonEvent'] = count_table['NonEvent'] / total_non_event\n",
    "\n",
    "        count_table['Dist_Event'] = count_table['Dist_Event'].replace(0, epsilon)\n",
    "        count_table['Dist_NonEvent'] = count_table['Dist_NonEvent'].replace(0, epsilon)\n",
    "\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "             count_table['WOE'] = np.log(count_table['Dist_Event'] / count_table['Dist_NonEvent'])\n",
    "\n",
    "        count_table.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "        count_table['IV'] = (count_table['Dist_Event'] - count_table['Dist_NonEvent']) * count_table['WOE']\n",
    "\n",
    "        feature_iv = count_table['IV'].sum()\n",
    "\n",
    "        return feature_iv\n",
    "\n",
    "    iv_dict = {col: woe_iv_optimized_single_feature(df, col, target)\n",
    "               for col in df.columns if col != target}\n",
    "\n",
    "    return pd.Series(iv_dict).sort_values(ascending=False)\n",
    "\n",
    "def select_strong_predictors(iv_series, threshold=0.02):\n",
    "    \"\"\"\n",
    "    Select predictors with strong IV above the threshold.\n",
    "\n",
    "    input:\n",
    "    - iv_series: Series with IV per variable\n",
    "    - threshold: Minimum IV to consider strong\n",
    "\n",
    "    output:\n",
    "    - list: Selected features\n",
    "    \"\"\"\n",
    "    return iv_series[iv_series >= threshold].index.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a49686",
   "metadata": {},
   "source": [
    "**Validando...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "75fa95ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_data.py\n",
    "\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from main_module import clean_data, calculate_iv_optimized, select_strong_predictors\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_df():\n",
    "    \"\"\"\n",
    "    Proporciona un dataframe de ejemplo limpio para pruebas.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'clump_thickness': [1, 2, 3, 4],\n",
    "        'uniformity_cell_size': [1, 2, 1, 2],\n",
    "        'bare_nuclei': [1, 2, 3, 4],\n",
    "        'class': [0, 1, 0, 1],\n",
    "        'id': [123, 456, 789, 101]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def test_clean_data(sample_df):\n",
    "    \"\"\"\n",
    "    Prueba la limpieza de datos, asegurando que la columna 'id' se elimine.\n",
    "    \"\"\"\n",
    "    cleaned = clean_data(sample_df)\n",
    "    assert 'id' not in cleaned.columns\n",
    "    assert cleaned['bare_nuclei'].dtype == int\n",
    "    assert set(cleaned['class'].unique()).issubset({0, 1})\n",
    "\n",
    "def test_calculate_iv_optimized(sample_df):\n",
    "    \"\"\"\n",
    "    Prueba el cálculo de IV asegurando que se obtengan valores positivos.\n",
    "    \"\"\"\n",
    "    df = clean_data(sample_df)\n",
    "    ivs = calculate_iv_optimized(df, 'class')\n",
    "    assert all(iv >= 0 for iv in ivs)\n",
    "\n",
    "def test_select_strong_predictors(sample_df):\n",
    "    \"\"\"\n",
    "    Prueba la selección de variables con IV mayor al umbral.\n",
    "    \"\"\"\n",
    "    df = clean_data(sample_df)\n",
    "    ivs = calculate_iv_optimized(df, 'class')\n",
    "    selected = select_strong_predictors(ivs, threshold=0.0)\n",
    "    assert len(selected) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1ad94e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.12.5, pytest-8.3.5, pluggy-1.5.0 -- C:\\Users\\AzShet\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\AzShet\\Documents\\Jupyter_LAB\\jupyter_projects\\5to_ciclo\\DataMining\\lab7\n",
      "plugins: anyio-4.3.0, dash-3.0.1\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 3 items\n",
      "\n",
      "test_data.py::test_clean_data \u001b[32mPASSED\u001b[0m\u001b[32m                                     [ 33%]\u001b[0m\n",
      "test_data.py::test_calculate_iv_optimized \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 66%]\u001b[0m\n",
      "test_data.py::test_select_strong_predictors \u001b[32mPASSED\u001b[0m\u001b[32m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 9.71s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_data.py -v\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
